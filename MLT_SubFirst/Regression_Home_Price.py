# -*- coding: utf-8 -*-
"""Copy of Bitch (2).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11hbWCr79oeyx_kCfGuwK1uEEh3DVAyZ8

# **ML Terapan - Submission 1 Predictive Analytics**
- **Nama:** Lutfi Aundrie Hermawan
- **Email:** lutfiaunher@gmail.com
- **ID Dicoding:** A126YBF254

## **Import Library**
"""

import warnings
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import (StandardScaler, MinMaxScaler, PolynomialFeatures, RobustScaler)
from sklearn.model_selection import (train_test_split, GridSearchCV, KFold, RandomizedSearchCV, StratifiedKFold, cross_val_score)
from sklearn.linear_model import (LinearRegression, Lasso, Ridge, ElasticNet, LassoCV, RidgeCV, ElasticNetCV)
from sklearn.feature_selection import RFE
from sklearn.metrics import (r2_score, mean_squared_error)
from sklearn.pipeline import make_pipeline
from sklearn.svm import SVR
from sklearn.ensemble import (GradientBoostingRegressor, RandomForestRegressor, AdaBoostRegressor, BaggingRegressor, ExtraTreesRegressor)
from xgboost import XGBRFRegressor, XGBRegressor
from lightgbm import LGBMRegressor
from mlxtend.regressor import StackingCVRegressor
from scipy.stats import skew
from sklearn.preprocessing import PowerTransformer

from sklearn.impute import SimpleImputer

# Setting options and context
warnings.filterwarnings('ignore')
sns.set_context("paper", font_scale=1, rc={"grid.linewidth": 3})
pd.set_option('display.max_rows', 100, 'display.max_columns', 400)

"""## **Mount to Drive**"""

from google.colab import drive
drive.mount('/content/drive')

"""## **Read Dataset**"""

df= pd.read_csv('/content/drive/MyDrive/Submission/Submission_One_ML_Terapan/USA Housing Dataset.csv')
df.head()

"""## **Exploratory Data Analysis**"""

df.info()

"""###**1. Check Missing Values**"""

print("\nMissing values dalam dataset:")
print(df.isnull().sum())

missing_num = df[df.columns].isna().sum().sort_values(ascending=False)
missing_perc = (df[df.columns].isna().sum()/len(df)*100).sort_values(ascending=False)
missing = pd.concat([missing_num,missing_perc],keys=['Total','Percentage'],axis=1)
missing_train = missing[missing['Percentage']>0]
missing_train

"""###**2. Distribution Visualisation**"""

# Define key numerical features
features = ['price', 'sqft_living', 'sqft_lot', 'sqft_above', 'sqft_basement', 'yr_built']

# Adjust the grid layout dynamically based on the number of features
num_features = len(features)
rows = (num_features // 3) + (num_features % 3 > 0)  # Calculate required rows
cols = 3  # 3 columns for better spacing

# Create subplots
fig, axes = plt.subplots(rows, cols, figsize=(18, 12))  # Adjust figure size for clarity
axes = axes.flatten()  # Flatten for easier indexing

# Create histograms for each feature
for i, feature in enumerate(features):
    sns.histplot(df[feature], bins=30, kde=True, ax=axes[i], color=sns.color_palette("viridis")[i])
    axes[i].set_title(f'Distribution of {feature}', fontsize=12, fontweight='bold')
    axes[i].set_xlabel(feature, fontsize=11)
    axes[i].set_ylabel('Frequency', fontsize=11)

# Hide any extra empty subplots
for j in range(i+1, len(axes)):
    fig.delaxes(axes[j])

# Optimize layout
plt.tight_layout()
plt.show()

"""###**3. Numerical and Categorical**"""

numerical = df.select_dtypes(include=['int64','float64']).drop(['price'],axis=1)
numerical.head()

categorical = df.select_dtypes(exclude=['int64','float64'])
categorical.head()

"""###**4. Visualisation Numerical**"""

#Visualising numerical predictor variables with Target Variables
train_num = df.select_dtypes(include=['int64','float64'])
# Calculate the number of rows needed for subplots
num_rows = int(np.ceil(len(train_num.columns) / 3))
# Create subplots with the calculated number of rows
fig, axs = plt.subplots(num_rows, 3, figsize=(20, 80))

#adjust horizontal space between plots
fig.subplots_adjust(hspace=0.6)
for i, ax in zip(train_num.columns, axs.flatten()):
    sns.scatterplot(x=i, y='price', hue='price', data=train_num, ax=ax, palette='viridis_r')
    plt.xlabel(i, fontsize=12)
    plt.ylabel('price', fontsize=12)
    ax.set_title('price' + ' - ' + str(i), fontweight='bold', size=20)

plt.show()

"""###**5. Visualisation Categorical**"""

def draw_boxplot(x, y, **kwargs):
    sns.boxplot(x=x, y=y)
    plt.xticks(rotation=90)

categorical_cols = df.select_dtypes(include=['object', 'category']).columns

melted_df = pd.melt(df, id_vars=['price'], value_vars=sorted(categorical_cols))

plot_grid = sns.FacetGrid(melted_df, col="variable", col_wrap=3, sharex=False, sharey=False, height=5)
plot_grid.map(draw_boxplot, "price", "value")

plt.tight_layout()
plt.show()

"""###**6. Outliers**"""

# Pilih fitur numerik
features = df.select_dtypes(include=['int64', 'float64'])

# Cek apakah ada fitur numerik
if not features.empty:
    # Tentukan jumlah subplot
    num_features = len(features.columns)
    cols = 3
    rows = (num_features // cols) + (num_features % cols > 0)

    # Buat subplots dengan grid
    fig, axes = plt.subplots(rows, cols, figsize=(18, 5 * rows))
    axes = axes.flatten()

    # Siapkan palet warna
    palette = sns.color_palette("viridis", n_colors=num_features)

    # Plot boxplot untuk tiap fitur
    for i, feature in enumerate(features.columns):
        sns.boxplot(x=df[feature], ax=axes[i], color=palette[i])
        axes[i].set_title(f'Box Plot of {feature}', fontsize=12, fontweight='bold')
        axes[i].set_xlabel(feature, fontsize=11)
        axes[i].grid(axis="x", linestyle="--", alpha=0.7)

    # Hilangkan subplot kosong
    for j in range(i + 1, len(axes)):
        fig.delaxes(axes[j])

    plt.tight_layout()
    plt.show()
else:
    print("DataFrame tidak memiliki fitur numerik.")

"""Terdeteksi outliers yaitu :
- price
- sqft_living
- sqft_lot
- sqft_above
- sqft_basement

###**7. Cek Skewness and Kurtosis**
"""

# Skew and kurtosis for SalePrice
print("Skewness: %f" % df['price'].skew())
print("Kurtosis: %f" % df['price'].kurt())

df_skew = df.select_dtypes(include=['int64','float64'])
skew_features = df_skew.apply(lambda x: skew(x)).sort_values(ascending=False)
high_skew = skew_features[skew_features > 0.5]
skew_index = high_skew.index
skewness = pd.DataFrame({'Skew' :high_skew})
skew_features

print(df[skew_index].min())

f, ax = plt.subplots(figsize=(8, 7))
ax.set_xscale("log")
ax = sns.boxplot(data=df_skew, orient="h", palette="Set1")
ax.xaxis.grid(False)
ax.set(ylabel="Feature names")
ax.set(xlabel="Numeric values")
ax.set(title="Numeric Distribution of Features")
sns.despine(trim=True, left=True)

"""###**8. Interquartile Range (IQR)**"""

# Columns for IQR filtering
columns_to_filter = ['price', 'sqft_living', 'sqft_lot', 'sqft_above','sqft_basement']

# Function to remove outliers using IQR
def remove_outliers_iqr(x, columns):
    for col in columns:
        Q1 = x[col].quantile(0.35)
        Q3 = x[col].quantile(0.85)
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR
        x = x[(x[col] >= lower_bound) & (x[col] <= upper_bound)]
    return x

# Apply IQR method
df = remove_outliers_iqr(df, columns_to_filter)

# Display the cleaned data
print(df.shape)

df["price"].value_counts().nlargest(10)

plt.figure(figsize=(10, 5))
sns.histplot(df['price'], bins=30, kde=True)  # KDE adds a smooth curve
plt.xlabel("Price")
plt.ylabel("Frequency")
plt.title("Distribution of House Prices")
plt.show()

"""### **9. Correlation Distribution**"""

#Correlation between variables to check multicollinearity
# Generate a mask for the upper triangle (taken from seaborn example gallery)
plt.subplots(figsize = (30,20))
# Changed np.bool to bool
mask = np.zeros_like(train_num.corr(), dtype=bool)
mask[np.triu_indices_from(mask)] = True
#Plotting heatmap
sns.heatmap(train_num.corr(), cmap=sns.diverging_palette(20, 220, n=200), mask = mask, annot=True, center = 0)

"""## **Data Preparation**

###**1. Hapus fitur yang tidak memberikan inforasi**
"""

# remove street country
df = df.drop(['street', 'country','statezip'], axis=1)

df.head()

#make the date column into year and month
df['date'] = pd.to_datetime(df['date'])
df['year_sold']= df['date'].dt.year
df['month_sold'] = df['date'].dt.month

#remove the date column now
df = df.drop('date', axis=1)

# remove year_sold
df = df.drop(['year_sold'],axis=1)

# Convert yr_renovated to 'Never Renovated' or 'Renovated'
df['renovation_status'] = df['yr_renovated'].apply(lambda x: 'Never_Renovated' if x == 0 else 'Renovated')
#also remove yr_renovated column
df = df.drop('yr_renovated', axis=1)

# Convert sqft_basement to 'Has Basement' or 'No Basement'
df['basement_status'] = df['sqft_basement'].apply(lambda x: 'No_Basement' if x == 0 else 'Has_Basement')
#also remove the sqft_basement after
df = df.drop('sqft_basement', axis=1)

# Price per Square Foot = price comparisons across different home sizes.
df['price_per_sqft'] = df['price'] / df['sqft_living']

df.head()

for col in df.columns:
    print(f"Column: {col}")
    print(df[col].value_counts().nlargest(10))  # Display only the top 10
    print("\n" + "-" * 50 + "\n")

"""###**2. Penanganan Skewness**"""

df.info()

df_skew = df.select_dtypes(include=['int64','float64'])
skew_features = df_skew.apply(lambda x: skew(x)).sort_values(ascending=False)
high_skew = skew_features[skew_features > 0.5]
skew_index = high_skew.index
skewness = pd.DataFrame({'Skew' :high_skew})
skew_features

df[skew_index].min()

transformer = PowerTransformer(method='yeo-johnson')
df_all = df
df_all[skew_index] = transformer.fit_transform(df_all[skew_index])

df_skew = df_all.select_dtypes(include=['int64','float64'])
skew_features = df_skew.apply(lambda x: skew(x)).sort_values(ascending=False)
high_skew = skew_features[skew_features > 0.5]
skew_index = high_skew.index
skewness = pd.DataFrame({'Skew' :high_skew})
skew_features

f, ax = plt.subplots(figsize=(8, 7))
ax.set_xscale("log")
ax = sns.boxplot(data=df_all, orient="h", palette="Set1")
ax.xaxis.grid(False)
ax.set(ylabel="Feature names")
ax.set(xlabel="Numeric values")
ax.set(title="Numeric Distribution of Features")
sns.despine(trim=True, left=True)

"""###**3. Ubah kolom categorical menggunakan one-hot encoding**"""

df_all_num= df_all.select_dtypes(include=['float64','int64']).columns  # Numerical columns
df_all_temp = df_all.select_dtypes(exclude=['float64','int64']) # selecting object and categorical features only
df_all_dummy= pd.get_dummies(df_all_temp)
df_all=pd.concat([df_all,df_all_dummy],axis=1) # joining converted dummy feature and original df_all dataset
df_all= df_all.drop(df_all_temp.columns,axis=1) #removing original categorical columns
df_all.shape

"""###**4. Cross Validation**"""

kfold= KFold(n_splits=11,random_state=42,shuffle=True) #kfold cross validation

# Misalkan 'price' adalah target variabel dan sisanya adalah fitur
X = df_all.drop('price', axis=1)  # Semua kolom kecuali 'price' sebagai fitur
y = df_all['price']  # Kolom 'price' sebagai target

# Error function to compute error
def rmsle(y, y_pred):
    return np.sqrt(mean_squared_error(y, y_pred))

#Assigning scoring paramter to 'neg_mean_squared_error' beacause 'mean_squared_error' is not
# available inside cross_val_score method
def cv_rmse(model, X=X):
    rmse = np.sqrt(-cross_val_score(model, X, y, scoring="neg_mean_squared_error", cv=kfold))
    return (rmse)

"""###**5. Splitting data into Trainand Test**"""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
print(X_train.shape)
print(y_train.shape)
print(X_test.shape)

"""##**Modelling**"""

# Menggunakan SimpleImputer untuk mengisi NaN dengan rata-rata kolom
imputer = SimpleImputer(strategy='mean')
X_train = imputer.fit_transform(X_train)
X_test = imputer.transform(X_test)

"""###**1. Ridge Regression**"""

ridge = Ridge()
params = {'alpha': [5, 8, 10, 10.1, 10.2, 10.3, 10.35, 10.36, 11, 12, 15]}
scaler = RobustScaler()

# Mengubah data
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Melakukan GridSearchCV
grid_ridge = GridSearchCV(ridge, param_grid=params, cv=kfold, scoring='neg_mean_squared_error')
grid_ridge.fit(X_train_scaled, y_train)

# Mendapatkan alpha terbaik
alpha = grid_ridge.best_params_['alpha']
ridge_score = -grid_ridge.best_score_  # Negasi karena scoring adalah negatif MSE

# Melatih model Ridge dengan alpha terbaik
ridge_alpha = Ridge(alpha=alpha)
ridge_alpha.fit(X_train_scaled, y_train)

# Melakukan prediksi
y_pred_train_ridge = ridge_alpha.predict(X_train_scaled)
y_pred_test_ridge = ridge_alpha.predict(X_test_scaled)

# Menghitung RMSE dan MSE
train_rmse = np.sqrt(mean_squared_error(y_train, y_pred_train_ridge))
test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test_ridge))
train_mse = mean_squared_error(y_train, y_pred_train_ridge)
test_mse = mean_squared_error(y_test, y_pred_test_ridge)

print(f"Ridge_MSE - Train: {train_mse}, Test: {test_mse}")
print(f"Ridge_RMSE - Train: {train_rmse}, Test: {test_rmse}")

"""###**2. Lasso Regression**"""

lasso = Lasso(alpha=0.001)  # Contoh alpha
lasso.fit(X_train_scaled, y_train)
y_pred_train_lasso = lasso.predict(X_train_scaled)
y_pred_test_lasso = lasso.predict(X_test_scaled)

lasso_mse_train = mean_squared_error(y_train, y_pred_train_lasso)
lasso_rmse_train = np.sqrt(lasso_mse_train)
lasso_mse_test = mean_squared_error(y_test, y_pred_test_lasso)
lasso_rmse_test = np.sqrt(lasso_mse_test)

print(f"Lasso_MSE - Train: {lasso_mse_train}, Test: {lasso_mse_test}")
print(f"Lasso_RMSE - Train: {lasso_rmse_train}, Test: {lasso_rmse_test}")

"""###**3. SVR**"""

svr = SVR(C=19, epsilon=0.008, gamma=0.00015)
svr.fit(X_train_scaled, y_train)
y_pred_train_svr = svr.predict(X_train_scaled)
y_pred_test_svr = svr.predict(X_test_scaled)

svr_mse_train = mean_squared_error(y_train, y_pred_train_svr)
svr_rmse_train = np.sqrt(svr_mse_train)
svr_mse_test = mean_squared_error(y_test, y_pred_test_svr)
svr_rmse_test = np.sqrt(svr_mse_test)

print(f"SVR_MSE - Train: {svr_mse_train}, Test: {svr_mse_test}")
print(f"SVR_RMSE - Train: {svr_rmse_train}, Test: {svr_rmse_test}")

"""###**4. ElasticNet Regression**"""

elasticnet = ElasticNet(alpha=0.001, l1_ratio=0.5)  # Contoh hyperparameter
elasticnet.fit(X_train_scaled, y_train)
y_pred_train_elastic = elasticnet.predict(X_train_scaled)
y_pred_test_elastic = elasticnet.predict(X_test_scaled)

elastic_mse_train = mean_squared_error(y_train, y_pred_train_elastic)
elastic_rmse_train = np.sqrt(elastic_mse_train)
elastic_mse_test = mean_squared_error(y_test, y_pred_test_elastic)
elastic_rmse_test = np.sqrt(elastic_mse_test)

print(f"ElasticNet_MSE - Train: {elastic_mse_train}, Test: {elastic_mse_test}")
print(f"ElasticNet_RMSE - Train: {elastic_rmse_train}, Test: {elastic_rmse_test}")

"""## Visualisasi Evaluasi"""

# Data MSE dan RMSE dari model yang sudah dievaluasi
model_names = ['Ridge', 'Lasso', 'ElasticNet', 'SVR']

# Data untuk visualisasi
train_mse = [train_mse, lasso_mse_train, elastic_mse_train, svr_mse_train]  # MSE dari training
train_rmse = [train_rmse, lasso_rmse_train, elastic_rmse_train, svr_rmse_train]  # RMSE dari training
test_mse = [test_mse, lasso_mse_test, elastic_mse_test, svr_mse_test]  # MSE dari testing
test_rmse = [test_rmse, lasso_rmse_test, elastic_rmse_test, svr_rmse_test]  # RMSE dari testing

# Membuat line chart
plt.figure(figsize=(14, 7))

# Plot untuk Train RMSE
plt.plot(model_names, train_rmse, marker='o', label='Train RMSE', linestyle='-', color='b')
# Plot untuk Test RMSE
plt.plot(model_names, test_rmse, marker='o', label='Test RMSE', linestyle='-', color='r')

# Plot untuk Train MSE
plt.plot(model_names, train_mse, marker='o', label='Train MSE', linestyle='--', color='c')
# Plot untuk Test MSE
plt.plot(model_names, test_mse, marker='o', label='Test MSE', linestyle='--', color='m')

# Menambahkan judul dan label
plt.title('Perbandingan Kinerja Model: RMSE dan MSE', fontsize=16)
plt.xlabel('Model', fontsize=14)
plt.ylabel('Error', fontsize=14)
plt.xticks(fontsize=12)
plt.yticks(fontsize=12)
plt.legend()
plt.grid(False)

# Menampilkan grafik
plt.show()